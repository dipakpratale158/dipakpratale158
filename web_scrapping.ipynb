{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73928827-5ccb-47bf-8bcb-030879b7bfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "#####################################################\n",
    "# Extracting the links of multiple movie transcripts\n",
    "#####################################################\n",
    "\n",
    "# How To Get The HTML\n",
    "root = 'https://subslikescript.com'  # this is the homepage of the website\n",
    "website = f'{root}/movies'  # concatenating the homepage with the movies section\n",
    "result = requests.get(website)\n",
    "content = result.text\n",
    "soup = BeautifulSoup(content, 'lxml')\n",
    "# print(soup.prettify())  # prints the HTML of the website\n",
    "\n",
    "# Locate the box that contains a list of movies\n",
    "box = soup.find('article', class_='main-article')\n",
    "\n",
    "# Store each link in \"links\" list (href doesn't consider root aka \"homepage\", so we have to concatenate it later)\n",
    "links = []\n",
    "for link in box.find_all('a', href=True):  # find_all returns a list\n",
    "    links.append(link['href'])\n",
    "\n",
    "#################################################\n",
    "# Extracting the movie transcript\n",
    "#################################################\n",
    "\n",
    "# Loop through the \"links\" list and sending a request to each link\n",
    "for link in links:\n",
    "    result = requests.get(f'{root}/{link}')\n",
    "    content = result.text\n",
    "    soup = BeautifulSoup(content, 'lxml')\n",
    "\n",
    "    # Locate the box that contains title and transcript\n",
    "    box = soup.find('article', class_='main-article')\n",
    "    # Locate title and transcript\n",
    "    title = box.find('h1').get_text()\n",
    "    title = ''.join(title.split('/'))\n",
    "    transcript = box.find('div', class_='full-script').get_text(strip=True, separator=' ')\n",
    "\n",
    "    # Exporting data in a text file with the \"title\" name\n",
    "    with open(f'{title}.txt', 'w') as file:\n",
    "        file.write(transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610147cc-d879-40e2-98d7-bb3ccafdef0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import json\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = 'quotes'\n",
    "    allowed_domains = ['quotes.toscrape.com']\n",
    "    start_urls = ['https://quotes.toscrape.com/api/quotes?page=1']\n",
    "\n",
    "    def parse(self, response):\n",
    "        # Storing the response in json and getting quotes\n",
    "        json_response = json.loads(response.body)\n",
    "        quotes = json_response.get('quotes')\n",
    "\n",
    "        # Looping through quote elements\n",
    "        for quote in quotes:\n",
    "            # Return data extracted\n",
    "            yield {\n",
    "                'author': quote.get('author').get('name'),\n",
    "                'tags': quote.get('tags'),\n",
    "                'quotes': quote.get('text'),\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c89e37-4456-44cb-8522-2d3c0e4df189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "\n",
    "class AudibleSpider(scrapy.Spider):\n",
    "    name = 'audible'\n",
    "    allowed_domains = ['www.audible.com']\n",
    "    start_urls = ['https://www.audible.com/search/']\n",
    "\n",
    "    def parse(self, response):\n",
    "        # Getting the box that contains all the info we want (title, author, length)\n",
    "        # product_container = response.xpath('//div[@class=\"adbl-impression-container \"]/li')\n",
    "        product_container = response.xpath('//div[@class=\"adbl-impression-container \"]//li[contains(@class, \"productListItem\")]')\n",
    "\n",
    "        # Looping through each product listed in the product_container box\n",
    "        for product in product_container:\n",
    "            book_title = product.xpath('.//h3[contains(@class , \"bc-heading\")]/a/text()').get()\n",
    "            book_author = product.xpath('.//li[contains(@class , \"authorLabel\")]/span/a/text()').getall()\n",
    "            book_length = product.xpath('.//li[contains(@class , \"runtimeLabel\")]/span/text()').get()\n",
    "\n",
    "            # Return data extracted\n",
    "            yield {\n",
    "                'title':book_title,\n",
    "                'author':book_author,\n",
    "                'length':book_length,\n",
    "            }\n",
    "\n",
    "        # Getting the pagination bar (pagination) and then the link within the next page button (next_page_url)\n",
    "        pagination = response.xpath('//ul[contains(@class , \"pagingElements\")]')\n",
    "        next_page_url = pagination.xpath('.//span[contains(@class , \"nextButton\")]/a/@href').get()\n",
    "        button_disabled = pagination.xpath('.//span[contains(@class , \"nextButton\")]/a/@aria-disabled').get()\n",
    "\n",
    "        # Going to the \"next_page_url\" link\n",
    "        if next_page_url and button_disabled==None:\n",
    "            yield response.follow(url=next_page_url, callback=self.parse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b230f40-b487-4f17-a7c5-8d9b56cfb501",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "from scrapy.spiders import CrawlSpider, Rule\n",
    "\n",
    "\n",
    "class TranscriptsSpider(CrawlSpider):\n",
    "    name = 'transcripts'\n",
    "    allowed_domains = ['subslikescript.com']\n",
    "    start_urls = ['https://subslikescript.com/movies_letter-X']  # let's test scraping all the pages for the X letter\n",
    "\n",
    "    # Setting rules for the crawler\n",
    "    rules = (\n",
    "        Rule(LinkExtractor(restrict_xpaths=(\"//ul[@class='scripts-list']/a\")), callback='parse_item', follow=True),\n",
    "        Rule(LinkExtractor(restrict_xpaths=(\"(//a[@rel='next'])[1]\"))),\n",
    "    )\n",
    "\n",
    "    def parse_item(self, response):\n",
    "        # Getting the article box that contains the data we want (title, plot, etc)\n",
    "        article = response.xpath(\"//article[@class='main-article']\")\n",
    "\n",
    "        # Extract the data we want and then yield it\n",
    "        yield {\n",
    "            'title':article.xpath(\"./h1/text()\").get(),\n",
    "            'plot':article.xpath(\"./p/text()\").get(),\n",
    "            # 'transcript':article.xpath(\"./div[@class='full-script']/text()\").getall(),\n",
    "            'url':response.url,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63db3a2f-14eb-43d7-9b7d-3e1259442e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy import FormRequest\n",
    "\n",
    "class QuotesLoginSpider(scrapy.Spider):\n",
    "    name = 'quotes_login'\n",
    "    allowed_domains = ['quotes.toscrape.com']\n",
    "    start_urls = ['https://quotes.toscrape.com/login']\n",
    "\n",
    "    # Parsing the csrf_token, username and password\n",
    "    def parse(self, response):\n",
    "        csrf_token = response.xpath(\"//input[@name='csrf_token']/@value\").get()\n",
    "        # sending FormRequest (FormRequest extends the base Request with functionality for dealing with HTML forms)\n",
    "        # FormRequest.from_response() simulates a user login\n",
    "        yield FormRequest.from_response(\n",
    "            response,\n",
    "            formxpath='//form',\n",
    "            formdata={\n",
    "                'csrf_token': csrf_token,\n",
    "                'username': 'admin',\n",
    "                'password': 'admin'\n",
    "            },\n",
    "            callback=self.after_login\n",
    "        )\n",
    "    # here we define the after_login function we used in callback\n",
    "    def after_login(self, response):\n",
    "        # If there's a \"logout\" text on the page, print \"Successfully logged in!\"\n",
    "        if response.xpath(\"//a[@href='/logout']/text()\").get():\n",
    "            print('Successfully logged in!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd72b46-d14d-4f45-95d1-af53054ed145",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import Select\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# define the website to scrape and path where the chromediver is located\n",
    "website = 'https://www.adamchoi.co.uk/overs/detailed'\n",
    "path = '/Users/frankandrade/Downloads/chromedriver'  # write your path here\n",
    "service = Service(executable_path=path)  # selenium 4\n",
    "driver = webdriver.Chrome(service=service)  # define 'driver' variable\n",
    "# open Google Chrome with chromedriver\n",
    "driver.get(website)\n",
    "\n",
    "# locate and click on a button\n",
    "all_matches_button = driver.find_element(by='xpath', value='//label[@analytics-event=\"All matches\"]')\n",
    "all_matches_button.click()\n",
    "\n",
    "# select dropdown and select element inside by visible text\n",
    "dropdown = Select(driver.find_element(by='id', value='country'))\n",
    "dropdown.select_by_visible_text('Spain')\n",
    "# implicit wait (useful in JavaScript driven websites when elements need seconds to load and avoid error \"ElementNotVisibleException\")\n",
    "time.sleep(3)\n",
    "\n",
    "# select elements in the table\n",
    "matches = driver.find_elements(by='xpath', value='//tr')\n",
    "\n",
    "# storage data in lists\n",
    "date = []\n",
    "home_team = []\n",
    "score = []\n",
    "away_team = []\n",
    "\n",
    "# looping through the matches list\n",
    "for match in matches:\n",
    "    date.append(match.find_element(by='xpath', value='./td[1]').text)\n",
    "    home = match.find_element(by='xpath', value='./td[2]').text\n",
    "    home_team.append(home)\n",
    "    print(home)\n",
    "    score.append(match.find_element(by='xpath', value='./td[3]').text)\n",
    "    away_team.append(match.find_element(by='xpath', value='./td[4]').text)\n",
    "\n",
    "# quit drive we opened at the beginning\n",
    "driver.quit()\n",
    "\n",
    "# Create Dataframe in Pandas and export to CSV (Excel)\n",
    "df = pd.DataFrame({'date': date, 'home_team': home_team, 'score': score, 'away_team': away_team})\n",
    "df.to_csv('football_data.csv', index=False)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81487e5c-0f0c-4cb7-8e73-90b3104afad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "\n",
    "web = \"https://twitter.com/search?q=python&src=typed_query\"\n",
    "path = '/Users/frank/Downloads/chromedriver'\n",
    "driver = webdriver.Chrome(path)\n",
    "driver.get(web)\n",
    "driver.maximize_window()\n",
    "\n",
    "\n",
    "def get_tweet(element):\n",
    "    \"\"\"This function scrapes data of tweets. It returns a list with 2 elements; username and text\"\"\"\n",
    "    try:\n",
    "        user = element.find_element_by_xpath(\".//span[contains(text(), '@')]\").text  # there are more than 1 but we pick the first\n",
    "        text = element.find_element_by_xpath(\".//div[@lang]\").text\n",
    "        tweets_data = [user, text]\n",
    "    except:\n",
    "        tweets_data = ['user', 'text']\n",
    "    return tweets_data\n",
    "\n",
    "\n",
    "# Initializing storage\n",
    "user_data = []\n",
    "text_data = []\n",
    "\n",
    "# Getting all the tweet cards/boxes listed in a single page\n",
    "tweets = WebDriverWait(driver, 5).until(EC.presence_of_all_elements_located((By.XPATH, \"//article[@role='article']\")))\n",
    "# Looping through the tweets list\n",
    "for tweet in tweets:\n",
    "    tweet_list = get_tweet(tweet)  # calling the function get_tweet to scrape data of the tweets list\n",
    "    user_data.append(tweet_list[0])  # appending the first element of tweet_list (user)\n",
    "    text_data.append(\" \".join(tweet_list[1].split()))  # appending the second element of tweet_list (text)\n",
    "\n",
    "driver.quit()\n",
    "# Storing the data into a DataFrame and exporting to a csv file\n",
    "df_tweets = pd.DataFrame({'user': user_data, 'text': text_data})\n",
    "df_tweets.to_csv('tweets.csv', index=False)\n",
    "print(df_tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6344503d-6c9c-453f-ac03-52c5e2906d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# The first website has 5 pages while the second has 60. Test the code with any of them\n",
    "web = \"https://www.audible.com/adblbestsellers?ref=a_search_t1_navTop_pl0cg1c0r0&pf_rd_p=adc4b13b-d074-4e1c-ac46-9f54aa53072b&pf_rd_r=1F7DV0MPHV77Z61RX566\"\n",
    "# web = \"https://www.audible.com/search\"\n",
    "path = '/Users/frankandrade/Downloads/chromedriver'\n",
    "driver = webdriver.Chrome(path)\n",
    "driver.get(web)\n",
    "driver.maximize_window()\n",
    "\n",
    "# Pagination 1\n",
    "pagination = driver.find_element_by_xpath('//ul[contains(@class, \"pagingElements\")]')  # locating pagination bar\n",
    "pages = pagination.find_elements_by_tag_name('li')  # locating each page displayed in the pagination bar\n",
    "last_page = int(pages[-2].text)  # getting the last page with negative indexing (starts from where the array ends)\n",
    "\n",
    "book_title = []\n",
    "book_author = []\n",
    "book_length = []\n",
    "\n",
    "# Pagination 2\n",
    "current_page = 1   # this is the page the bot starts scraping\n",
    "\n",
    "# The while loop below will work until the the bot reaches the last page of the website, then it will break\n",
    "while current_page <= last_page:\n",
    "    time.sleep(2)  # let the page render correctly\n",
    "    container = driver.find_element_by_class_name('adbl-impression-container ')\n",
    "    products = container.find_elements_by_xpath('.//li[contains(@class, \"productListItem\")]')\n",
    "    # products = container.find_elements_by_xpath('./li')\n",
    "\n",
    "    for product in products:\n",
    "        book_title.append(product.find_element_by_xpath('.//h3[contains(@class, \"bc-heading\")]').text)\n",
    "        book_author.append(product.find_element_by_xpath('.//li[contains(@class, \"authorLabel\")]').text)\n",
    "        book_length.append(product.find_element_by_xpath('.//li[contains(@class, \"runtimeLabel\")]').text)\n",
    "\n",
    "    current_page = current_page + 1  # increment the current_page by 1 after the data is extracted\n",
    "    # Locating the next_page button and clicking on it. If the element isn't on the website, pass to the next iteration\n",
    "    try:\n",
    "        next_page = driver.find_element_by_xpath('.//span[contains(@class , \"nextButton\")]')\n",
    "        next_page.click()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "df_books = pd.DataFrame({'title': book_title, 'author': book_author, 'length': book_length})\n",
    "df_books.to_csv('books_pagination.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1ffd2e-3c80-455f-a5a1-fafb43035bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "#####################################################\n",
    "# Extracting links from pagination bar\n",
    "#####################################################\n",
    "\n",
    "# How To Get The HTML\n",
    "root = 'https://subslikescript.com'  # this is the homepage of the website\n",
    "website = f'{root}/movies_letter-X'  # concatenating the homepage with the movies \"letter-X\" section. You can choose any section (e.g., letter-A, letter-B, ...)\n",
    "result = requests.get(website)\n",
    "content = result.text\n",
    "soup = BeautifulSoup(content, 'lxml')\n",
    "\n",
    "# Locate the box that contains the pagination bar\n",
    "pagination = soup.find('ul', class_='pagination')\n",
    "pages = pagination.find_all('li', class_='page-item')\n",
    "last_page = pages[-2].text  # this is the number of pages that the website has inside the movies \"letter X\" section\n",
    "\n",
    "##################################################################################\n",
    "# Extracting the links of multiple movie transcripts inside each page listed\n",
    "##################################################################################\n",
    "\n",
    "# Loop through all tbe pages and sending a request to each link\n",
    "for page in range(1, int(last_page)+1):\n",
    "    result = requests.get(f'{website}?page={page}')  # structure --> https://subslikescript.com/movies_letter-X?page=2\n",
    "    content = result.text\n",
    "    soup = BeautifulSoup(content, 'lxml')\n",
    "\n",
    "    # Locate the box that contains a list of movies\n",
    "    box = soup.find('article', class_='main-article')\n",
    "\n",
    "    # Store each link in \"links\" list (href doesn't consider root aka \"homepage\", so we have to concatenate it later)\n",
    "    links = []\n",
    "    for link in box.find_all('a', href=True):  # find_all returns a list\n",
    "        links.append(link['href'])\n",
    "\n",
    "    #################################################\n",
    "    # Extracting the movie transcript\n",
    "    #################################################\n",
    "\n",
    "    for link in links:\n",
    "        try:  # \"try the code below. if something goes wrong, go to the \"except\" block\"\n",
    "            result = requests.get(f'{root}/{link}')  # structure --> https://subslikescript.com/movie/X-Men_2-290334\n",
    "            content = result.text\n",
    "            soup = BeautifulSoup(content, 'lxml')\n",
    "\n",
    "            # Locate the box that contains title and transcript\n",
    "            box = soup.find('article', class_='main-article')\n",
    "            # Locate title and transcript\n",
    "            title = box.find('h1').get_text()\n",
    "            transcript = box.find('div', class_='full-script').get_text(strip=True, separator=' ')\n",
    "\n",
    "            # Exporting data in a text file with the \"title\" name\n",
    "            with open(f'{title}.txt', 'w') as file:\n",
    "                file.write(transcript)\n",
    "        except:\n",
    "            print('------ Link not working -------')\n",
    "            print(link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f694cb-dd1e-44bb-8e65-d9bb8ae23c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import json\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = 'quotes'\n",
    "    allowed_domains = ['quotes.toscrape.com']\n",
    "    start_urls = ['https://quotes.toscrape.com/api/quotes?page=1']\n",
    "\n",
    "    def parse(self, response):\n",
    "        # Storing the response in json and getting quotes\n",
    "        json_response = json.loads(response.body)\n",
    "        quotes = json_response.get('quotes')\n",
    "\n",
    "        # Looping through quote elements\n",
    "        for quote in quotes:\n",
    "            # Return data extracted\n",
    "            yield {\n",
    "                'author': quote.get('author').get('name'),\n",
    "                'tags': quote.get('tags'),\n",
    "                'quotes': quote.get('text'),\n",
    "            }\n",
    "\n",
    "        # Picking the \"has_next\" element\n",
    "        has_next = json_response.get('has_next')\n",
    "\n",
    "        # If has_next==True (there's next page), execute the following code\n",
    "        if has_next:\n",
    "            next_page_number = json_response.get('page')+1\n",
    "            yield scrapy.Request(\n",
    "                url=f'https://quotes.toscrape.com/api/quotes?page={next_page_number}',\n",
    "                callback=self.parse\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff352caf-e6d2-4600-bee5-63524d859bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "\n",
    "class AudibleSpider(scrapy.Spider):\n",
    "    name = 'audible'\n",
    "    allowed_domains = ['www.audible.com']\n",
    "    start_urls = ['https://www.audible.com/search/']\n",
    "\n",
    "    def start_requests(self):\n",
    "        # Editing the default headers (user-agent)\n",
    "        yield scrapy.Request(url='https://www.audible.com/search/', callback=self.parse,\n",
    "                       headers={'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36'})\n",
    "\n",
    "    def parse(self, response):\n",
    "        # Getting the box that contains all the info we want (title, author, length)\n",
    "        # product_container = response.xpath('//div[@class=\"adbl-impression-container \"]/li')\n",
    "        product_container = response.xpath('//div[@class=\"adbl-impression-container \"]//li[contains(@class, \"productListItem\")]')\n",
    "\n",
    "        # Looping through each product listed in the product_container box\n",
    "        for product in product_container:\n",
    "            book_title = product.xpath('.//h3[contains(@class , \"bc-heading\")]/a/text()').get()\n",
    "            book_author = product.xpath('.//li[contains(@class , \"authorLabel\")]/span/a/text()').getall()\n",
    "            book_length = product.xpath('.//li[contains(@class , \"runtimeLabel\")]/span/text()').get()\n",
    "\n",
    "            # Return data extracted and also the user-agent defined before\n",
    "            yield {\n",
    "                'title':book_title,\n",
    "                'author':book_author,\n",
    "                'length':book_length,\n",
    "                'User-Agent':response.request.headers['User-Agent'],\n",
    "            }\n",
    "\n",
    "        # Getting the pagination bar (pagination) and then the link within the next page button (next_page_url)\n",
    "        pagination = response.xpath('//ul[contains(@class , \"pagingElements\")]')\n",
    "        next_page_url = pagination.xpath('.//span[contains(@class , \"nextButton\")]/a/@href').get()\n",
    "        button_disabled = pagination.xpath('.//span[contains(@class , \"nextButton\")]/a/@aria-disabled').get()\n",
    "\n",
    "        # Going to the \"next_page_url\" link using the user-agent defined before\n",
    "        if next_page_url and button_disabled==None:\n",
    "            yield response.follow(url=next_page_url, callback=self.parse,\n",
    "                                  headers={'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4deefe0-c922-458d-ae59-0d829a4835a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "\n",
    "web = \"https://twitter.com/TwitterSupport/status/1415364740583395328\"\n",
    "# web = \"https://twitter.com/TwitterSupport\"\n",
    "path = '/Users/frank/Downloads/chromedriver'\n",
    "driver = webdriver.Chrome(path)\n",
    "driver.get(web)\n",
    "driver.maximize_window()\n",
    "\n",
    "# Get the initial scroll height\n",
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "while True:\n",
    "    # Scroll down to bottom\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    # Wait to load page\n",
    "    time.sleep(5)\n",
    "    # Calculate new scroll height and compare it with last scroll height\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    if new_height == last_height:  # if the new and last height are equal, it means that there isn't any new page to load, so we stop scrolling\n",
    "        break\n",
    "    else:\n",
    "        last_height = new_height\n",
    "\n",
    "# def get_tweet(element):\n",
    "#     try:\n",
    "#         user = element.find_element_by_xpath(\".//span[contains(text(), '@')]\").text\n",
    "#         text = element.find_element_by_xpath(\".//div[@lang]\").text\n",
    "#         tweet_data = [user, text]\n",
    "#     except:\n",
    "#         tweet_data = ['user', 'text']\n",
    "#     return tweet_data\n",
    "\n",
    "# user_data = []\n",
    "# text_data = []\n",
    "#\n",
    "# tweets = WebDriverWait(driver, 5).until(EC.presence_of_all_elements_located((By.XPATH, \"//article[@role='article']\")))\n",
    "# for tweet in tweets:\n",
    "#     tweet_list = get_tweet(tweet)\n",
    "#     user_data.append(tweet_list[0])\n",
    "#     text_data.append(\" \".join(tweet_list[1].split()))\n",
    "\n",
    "driver.quit()\n",
    "#\n",
    "# df_tweets = pd.DataFrame({'user': user_data, 'text': text_data})\n",
    "# df_tweets.to_csv('tweets.csv', index=False)\n",
    "# print(df_tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d28287-eb0d-4cb2-af7d-a5806dceff61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "from scrapy.spiders import CrawlSpider, Rule\n",
    "\n",
    "\n",
    "class TranscriptsSpider(CrawlSpider):\n",
    "    name = 'transcripts'\n",
    "    allowed_domains = ['subslikescript.com']\n",
    "    # start_urls = ['https://subslikescript.com/movies_letter-X']\n",
    "\n",
    "    # Setting an user-agent variable\n",
    "    user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36'\n",
    "\n",
    "    # Editing the user-agent in the request sent\n",
    "    def start_requests(self):\n",
    "        yield scrapy.Request(url='https://subslikescript.com/movies_letter-X', headers={\n",
    "            'user-agent':self.user_agent\n",
    "        })\n",
    "\n",
    "    # Setting rules for the crawler\n",
    "    rules = (\n",
    "        Rule(LinkExtractor(restrict_xpaths=(\"//ul[@class='scripts-list']/a\")), callback='parse_item', follow=True, process_request='set_user_agent'),\n",
    "        Rule(LinkExtractor(restrict_xpaths=(\"(//a[@rel='next'])[1]\")), process_request='set_user_agent'),\n",
    "    )\n",
    "\n",
    "    # Setting the user-agent\n",
    "    def set_user_agent(self, request, spider):\n",
    "        request.headers['User-Agent'] = self.user_agent\n",
    "        return request\n",
    "\n",
    "    def parse_item(self, response):\n",
    "        # Getting the article box that contains the data we want (title, plot, etc)\n",
    "        article = response.xpath(\"//article[@class='main-article']\")\n",
    "\n",
    "        # Extract the data we want and then yield it\n",
    "        yield {\n",
    "            'title': article.xpath(\"./h1/text()\").get(),\n",
    "            'plot': article.xpath(\"./p/text()\").get(),\n",
    "            'transcript': article.xpath(\"./div[@class='full-script']/text()\").getall(),\n",
    "            'url': response.url,\n",
    "            'user-agent': response.request.headers['User-Agent'],\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f25a9b-694b-48e1-9342-2249818bb287",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "\n",
    "web = \"https://twitter.com/TwitterSupport/status/1415364740583395328\"\n",
    "# web = \"https://twitter.com/TwitterSupport\"\n",
    "path = '/Users/frank/Downloads/chromedriver'\n",
    "driver = webdriver.Chrome(path)\n",
    "driver.get(web)\n",
    "driver.maximize_window()\n",
    "\n",
    "def get_tweet(element):\n",
    "    try:\n",
    "        user = element.find_element_by_xpath(\".//span[contains(text(), '@')]\").text\n",
    "        text = element.find_element_by_xpath(\".//div[@lang]\").text\n",
    "        tweet_data = [user, text]\n",
    "    except:\n",
    "        tweet_data = ['user', 'text']\n",
    "    return tweet_data\n",
    "\n",
    "\n",
    "user_data = []\n",
    "text_data = []\n",
    "tweet_ids = set()\n",
    "scrolling = True\n",
    "while scrolling:\n",
    "    tweets = WebDriverWait(driver, 5).until(\n",
    "        EC.presence_of_all_elements_located((By.XPATH, \"//article[@role='article']\")))\n",
    "    print(len(tweets))\n",
    "    for tweet in tweets[-15:]:  # you can change this number with the number of tweets in a website || NOTE: ONLY THOSE LOADED IN THE last page will be considered while those from previous page will be forgotten (example: scroll all the way down and then try to find an @username that it's on top --> it won't find it)\n",
    "        tweet_list = get_tweet(tweet)\n",
    "        tweet_id = ''.join(tweet_list)\n",
    "        if tweet_id not in tweet_ids:\n",
    "            tweet_ids.add(tweet_id)\n",
    "            user_data.append(tweet_list[0])\n",
    "            text_data.append(\" \".join(tweet_list[1].split()))\n",
    "\n",
    "    # Get the initial scroll height\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    while True:\n",
    "        # Scroll down to bottom\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        # Wait to load page\n",
    "        time.sleep(2)\n",
    "        # Calculate new scroll height and compare it with last scroll height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        # condition 1\n",
    "        if new_height == last_height:  # if the new and last height are equal, it means that there isn't any new page to load, so we stop scrolling\n",
    "            scrolling = False\n",
    "            break\n",
    "        # condition 2\n",
    "        # if len(data) > 60:\n",
    "        #     scrolling = False\n",
    "        #     break\n",
    "        else:\n",
    "            last_height = new_height\n",
    "            break\n",
    "\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "df_tweets = pd.DataFrame({'user': user_data, 'text': text_data})\n",
    "df_tweets.to_csv('tweets_pagination.csv', index=False)\n",
    "print(df_tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0532f61b-8c37-4e3c-bd2a-184516d6dd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "\n",
    "class WorldometersSpider(scrapy.Spider):\n",
    "    name = 'worldometers'\n",
    "    allowed_domains = ['www.worldometers.info/']\n",
    "    start_urls = ['https://www.worldometers.info/world-population/population-by-country/']\n",
    "\n",
    "    def parse(self, response):\n",
    "        # Extracting title and country names\n",
    "        title = response.xpath('//h1/text()').get()\n",
    "        countries = response.xpath('//td/a/text()').getall()\n",
    "\n",
    "        # return data extracted\n",
    "        yield {\n",
    "            'titles': title,\n",
    "            'countries': countries,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3adc840-7a97-494c-9964-f555f9c95f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "\n",
    "class WorldometersSpider(scrapy.Spider):\n",
    "    name = 'worldometers'\n",
    "    allowed_domains = ['www.worldometers.info/']\n",
    "    start_urls = ['https://www.worldometers.info/world-population/population-by-country/']\n",
    "\n",
    "    def parse(self, response):\n",
    "        # Extracting \"a\" elements for each country\n",
    "        countries = response.xpath('//td/a')\n",
    "\n",
    "        # Looping through the countries list\n",
    "        for country in countries:\n",
    "            country_name = country.xpath(\".//text()\").get()\n",
    "            link = country.xpath(\".//@href\").get()\n",
    "\n",
    "            # return data extracted\n",
    "            yield {\n",
    "                'country_name': country_name,\n",
    "                'link': link,\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108d00e8-393b-4430-a745-abdaffa0086b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "\n",
    "class WorldometersSpider(scrapy.Spider):\n",
    "    name = 'worldometers'\n",
    "    allowed_domains = ['www.worldometers.info']\n",
    "    start_urls = ['https://www.worldometers.info/world-population/population-by-country/']\n",
    "\n",
    "    def parse(self, response):\n",
    "        # Extracting \"a\" elements for each country\n",
    "        countries = response.xpath('//td/a')\n",
    "\n",
    "        # Looping through the countries list\n",
    "        for country in countries:\n",
    "            country_name = country.xpath(\".//text()\").get()\n",
    "            link = country.xpath(\".//@href\").get()\n",
    "\n",
    "            # Absolute URL\n",
    "            # absolute_url = f'https://www.worldometers.info/{link}'  # concatenating links with f-string\n",
    "            # absolute_url = response.urljoin(link)  # concatenating links with urljoin\n",
    "            # yield scrapy.Request(url=absolute_url) # sending a request with the absolute url\n",
    "\n",
    "            # Return relative URL\n",
    "            yield response.follow(url=link)  # sending a request with the relative url\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9b20e8-c579-42f6-a56a-4548da44695f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "\n",
    "class WorldometersSpider(scrapy.Spider):\n",
    "    name = 'worldometers'\n",
    "    allowed_domains = ['www.worldometers.info']\n",
    "    start_urls = ['https://www.worldometers.info/world-population/population-by-country/']\n",
    "\n",
    "    def parse(self, response):\n",
    "        # Extracting \"a\" elements for each country\n",
    "        countries = response.xpath('//td/a')\n",
    "\n",
    "        # Looping through the countries list\n",
    "        for country in countries:\n",
    "            country_name = country.xpath(\".//text()\").get()\n",
    "            link = country.xpath(\".//@href\").get()\n",
    "\n",
    "            # Absolute URL\n",
    "            # absolute_url = f'https://www.worldometers.info/{link}'  # concatenating links with f-string\n",
    "            # absolute_url = response.urljoin(link)  # concatenating links with urljoin\n",
    "            # yield scrapy.Request(url=absolute_url)  # sending a request with the absolute url\n",
    "\n",
    "            # Return relative URL (sending a request with the relative url)\n",
    "            yield response.follow(url=link, callback=self.parse_country, meta={'country':country_name})\n",
    "\n",
    "    # Getting data inside the \"link\" website\n",
    "    def parse_country(self, response):\n",
    "        # Getting country names and each row element inside the population table\n",
    "        country = response.request.meta['country']\n",
    "        rows = response.xpath(\"(//table[contains(@class,'table')])[1]/tbody/tr\")  # You can also use the whole class value  --> response.xpath('(//table[@class=\"table table-striped table-bordered table-hover table-condensed table-list\"])[1]/tbody/tr')\n",
    "        # Looping through the rows list\n",
    "        for row in rows:\n",
    "            year = row.xpath(\".//td[1]/text()\").get()\n",
    "            population = row.xpath(\".//td[2]/strong/text()\").get()\n",
    "\n",
    "            # Return data extracted\n",
    "            yield {\n",
    "                'country':country,\n",
    "                'year': year,\n",
    "                'population':population,\n",
    "            }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eace29d-8ddb-49f9-8195-c8cab50f3983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your item pipelines here\n",
    "#\n",
    "# Don't forget to add your pipeline to the ITEM_PIPELINES setting\n",
    "# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n",
    "\n",
    "\n",
    "# useful for handling different item types with a single interface\n",
    "from itemadapter import ItemAdapter\n",
    "\n",
    "import logging\n",
    "import pymongo\n",
    "import sqlite3\n",
    "\n",
    "class MongodbPipeline:\n",
    "    collection_name = 'transcripts'\n",
    "\n",
    "    def open_spider(self, spider):\n",
    "        self.client = pymongo.MongoClient(\"mongodb+srv://frank:frank@cluster0.m0o4d.mongodb.net/myFirstDatabase?retryWrites=true&w=majority\")\n",
    "        self.db = self.client['My_Database']\n",
    "\n",
    "    def close_spider(self, spider):\n",
    "        self.client.close()\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        self.db[self.collection_name].insert(item)\n",
    "        return item\n",
    "\n",
    "class SQLitePipeline:\n",
    "\n",
    "    def open_spider(self, spider):\n",
    "        # create database file\n",
    "        self.connection = sqlite3.connect('transcripts.db')\n",
    "        # we need a cursor object to execute SQL queries\n",
    "        self.c = self.connection.cursor()\n",
    "        #  try/except will help when running this for the +2nd time (we can't create the same table twice)\n",
    "        try:\n",
    "            # query: create table with columns\n",
    "            self.c.execute('''\n",
    "                CREATE TABLE transcripts(\n",
    "                    title TEXT,\n",
    "                    plot TEXT,\n",
    "                    transcript TEXT,\n",
    "                    url TEXT\n",
    "                )\n",
    "            ''')\n",
    "            # save changes\n",
    "            self.connection.commit()\n",
    "        except sqlite3.OperationalError:\n",
    "            pass\n",
    "\n",
    "\n",
    "    def close_spider(self, spider):\n",
    "        self.connection.close()\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        # query: insert data into table\n",
    "        self.c.execute('''\n",
    "            INSERT INTO transcripts (title,plot,transcript,url) VALUES(?,?,?,?)\n",
    "        ''', (\n",
    "            item.get('title'),\n",
    "            item.get('plot'),\n",
    "            item.get('transcript'),\n",
    "            item.get('url'),\n",
    "        ))\n",
    "        # save changes\n",
    "        self.connection.commit()\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37355a74-f46d-4bc5-b8b7-cc17c3131f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrapy settings for spider_tutorial project\n",
    "#\n",
    "# For simplicity, this file contains only settings considered important or\n",
    "# commonly used. You can find more settings consulting the documentation:\n",
    "#\n",
    "#     https://docs.scrapy.org/en/latest/topics/settings.html\n",
    "#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n",
    "#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n",
    "\n",
    "BOT_NAME = 'spider_tutorial'\n",
    "\n",
    "SPIDER_MODULES = ['spider_tutorial.spiders']\n",
    "NEWSPIDER_MODULE = 'spider_tutorial.spiders'\n",
    "\n",
    "\n",
    "# Crawl responsibly by identifying yourself (and your website) on the user-agent\n",
    "# USER_AGENT = 'spider_tutorial (+http://www.yourdomain.com)'\n",
    "\n",
    "# Obey robots.txt rules\n",
    "ROBOTSTXT_OBEY = True\n",
    "\n",
    "# Configure maximum concurrent requests performed by Scrapy (default: 16)\n",
    "#CONCURRENT_REQUESTS = 32\n",
    "\n",
    "# Configure a delay for requests for the same website (default: 0)\n",
    "# See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay\n",
    "# See also autothrottle settings and docs\n",
    "#DOWNLOAD_DELAY = 3\n",
    "# The download delay setting will honor only one of:\n",
    "#CONCURRENT_REQUESTS_PER_DOMAIN = 16\n",
    "#CONCURRENT_REQUESTS_PER_IP = 16\n",
    "\n",
    "# Disable cookies (enabled by default)\n",
    "#COOKIES_ENABLED = False\n",
    "\n",
    "# Disable Telnet Console (enabled by default)\n",
    "#TELNETCONSOLE_ENABLED = False\n",
    "\n",
    "# Override the default request headers:\n",
    "# DEFAULT_REQUEST_HEADERS = {\n",
    "#   'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36'\n",
    "# }\n",
    "\n",
    "# Enable or disable spider middlewares\n",
    "# See https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n",
    "#SPIDER_MIDDLEWARES = {\n",
    "#    'spider_tutorial.middlewares.SpiderTutorialSpiderMiddleware': 543,\n",
    "#}\n",
    "\n",
    "# Enable or disable downloader middlewares\n",
    "# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n",
    "#DOWNLOADER_MIDDLEWARES = {\n",
    "#    'spider_tutorial.middlewares.SpiderTutorialDownloaderMiddleware': 543,\n",
    "#}\n",
    "\n",
    "# Enable or disable extensions\n",
    "# See https://docs.scrapy.org/en/latest/topics/extensions.html\n",
    "#EXTENSIONS = {\n",
    "#    'scrapy.extensions.telnet.TelnetConsole': None,\n",
    "#}\n",
    "\n",
    "# Configure item pipelines\n",
    "# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n",
    "ITEM_PIPELINES = {\n",
    "   'spider_tutorial.pipelines.SQLitePipeline': 300,\n",
    "}\n",
    "\n",
    "# Enable and configure the AutoThrottle extension (disabled by default)\n",
    "# See https://docs.scrapy.org/en/latest/topics/autothrottle.html\n",
    "#AUTOTHROTTLE_ENABLED = True\n",
    "# The initial download delay\n",
    "#AUTOTHROTTLE_START_DELAY = 5\n",
    "# The maximum download delay to be set in case of high latencies\n",
    "#AUTOTHROTTLE_MAX_DELAY = 60\n",
    "# The average number of requests Scrapy should be sending in parallel to\n",
    "# each remote server\n",
    "#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\n",
    "# Enable showing throttling stats for every response received:\n",
    "#AUTOTHROTTLE_DEBUG = False\n",
    "\n",
    "# Enable and configure HTTP caching (disabled by default)\n",
    "# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings\n",
    "#HTTPCACHE_ENABLED = True\n",
    "#HTTPCACHE_EXPIRATION_SECS = 0\n",
    "#HTTPCACHE_DIR = 'httpcache'\n",
    "#HTTPCACHE_IGNORE_HTTP_CODES = []\n",
    "#HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'\n",
    "\n",
    "FEED_EXPORT_ENCODING = 'utf-8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b5da37-01c8-4dfa-9110-b863a7cb24d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import pandas as pd\n",
    "\n",
    "# Headless mode\n",
    "options = Options()  # Initialize an instance of the Options class\n",
    "options.headless = True  # True -> Headless mode activated\n",
    "options.add_argument('window-size=1920x1080')  # Set a big window size, so all the data will be displayed\n",
    "\n",
    "web = \"https://www.audible.com/search\"\n",
    "path = '/Users/frankandrade/Downloads/chromedriver'\n",
    "driver = webdriver.Chrome(path, options=options)  # add the \"options\" argument to make sure the changes are applied\n",
    "driver.get(web)\n",
    "# driver.maximize_window()\n",
    "\n",
    "container = driver.find_element_by_class_name('adbl-impression-container ')\n",
    "products = container.find_elements_by_xpath('.//li[contains(@class, \"productListItem\")]')\n",
    "# products = container.find_elements_by_xpath('./li')\n",
    "\n",
    "book_title = []\n",
    "book_author = []\n",
    "book_length = []\n",
    "\n",
    "for product in products:\n",
    "    # In headless mode we won't see the bot scraping the website, so print any element to check the progress\n",
    "    title = product.find_element_by_xpath('.//h3[contains(@class, \"bc-heading\")]').text\n",
    "    book_title.append(title)\n",
    "    print(title)\n",
    "    book_author.append(product.find_element_by_xpath('.//li[contains(@class, \"authorLabel\")]').text)\n",
    "    book_length.append(product.find_element_by_xpath('.//li[contains(@class, \"runtimeLabel\")]').text)\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "df_books = pd.DataFrame({'title': book_title, 'author': book_author, 'length': book_length})\n",
    "df_books.to_csv('books_headless_mode.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565bd4b3-6dcd-44df-b9a8-91411d570185",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "import os\n",
    "\n",
    "web = \"https://twitter.com/i/flow/login\"\n",
    "path = \"/Users/frankandrade/Downloads/chromedriver\"\n",
    "driver = webdriver.Chrome(path)\n",
    "driver.get(web)\n",
    "driver.maximize_window()\n",
    "\n",
    "# wait of 6 seconds to let the page load the content\n",
    "time.sleep(6)  # this time might vary depending on your computer\n",
    "\n",
    "# locating username and password inputs and sending text to the inputs\n",
    "# username\n",
    "username = driver.find_element_by_xpath('//input[@autocomplete =\"username\"]')\n",
    "username.send_keys(\"my_username\")  # Write Email Here\n",
    "# username.send_keys(os.environ.get(\"TWITTER_USER\"))\n",
    "\n",
    "# Clicking on \"Next\" button\n",
    "next_button = driver.find_element_by_xpath('//div[@role=\"button\"]//span[text()=\"Next\"]')\n",
    "next_button.click()\n",
    "\n",
    "# wait of 2 seconds after clicking button\n",
    "time.sleep(2)\n",
    "\n",
    "# password\n",
    "password = driver.find_element_by_xpath('//input[@autocomplete =\"current-password\"]')\n",
    "password.send_keys(\"my_password\")  # Write Password Here\n",
    "# password.send_keys(os.environ.get(\"TWITTER_PASS\"))\n",
    "\n",
    "# locating login button and then clicking on it\n",
    "login_button = driver.find_element_by_xpath('//div[@role=\"button\"]//span[text()=\"Log in\"]')\n",
    "login_button.click()\n",
    "\n",
    "# closing driver\n",
    "# driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b9bddd-9bab-4545-9403-19f0e4d6ace0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "import os\n",
    "\n",
    "web = \"https://twitter.com/i/flow/login\"\n",
    "path = \"/Users/frankandrade/Downloads/chromedriver\"\n",
    "driver = webdriver.Chrome(path)\n",
    "driver.get(web)\n",
    "driver.maximize_window()\n",
    "\n",
    "# wait of 6 seconds to let the page load the content\n",
    "time.sleep(6)  # this time might vary depending on your computer\n",
    "\n",
    "# locating username and password inputs and sending text to the inputs\n",
    "# username\n",
    "username = driver.find_element_by_xpath('//input[@autocomplete =\"username\"]')\n",
    "username.send_keys(\"my_username\")  # Write Email Here\n",
    "# username.send_keys(os.environ.get(\"TWITTER_USER\"))\n",
    "\n",
    "# Clicking on \"Next\" button\n",
    "next_button = driver.find_element_by_xpath('//div[@role=\"button\"]//span[text()=\"Next\"]')\n",
    "next_button.click()\n",
    "\n",
    "# wait of 2 seconds after clicking button\n",
    "time.sleep(2)\n",
    "\n",
    "# password\n",
    "password = driver.find_element_by_xpath('//input[@autocomplete =\"current-password\"]')\n",
    "password.send_keys(\"my_password\")  # Write Password Here\n",
    "# password.send_keys(os.environ.get(\"TWITTER_PASS\"))\n",
    "\n",
    "# locating login button and then clicking on it\n",
    "login_button = driver.find_element_by_xpath('//div[@role=\"button\"]//span[text()=\"Log in\"]')\n",
    "login_button.click()\n",
    "\n",
    "# closing driver\n",
    "# driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b1d7b8-a8ea-44de-9388-58b697cff44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "import os\n",
    "\n",
    "web = \"https://twitter.com/\"\n",
    "path = \"/Users/frank/Downloads/chromedriver\"\n",
    "driver = webdriver.Chrome(path)\n",
    "driver.get(web)\n",
    "driver.maximize_window()\n",
    "\n",
    "# locating and clicking the login button\n",
    "login = driver.find_element_by_xpath('//a[@href=\"/login\"]')\n",
    "login.click()\n",
    "time.sleep(2)\n",
    "\n",
    "# getting the login box that contains the username and password\n",
    "login_box = driver.find_element_by_xpath('//form[@action=\"/sessions\"]')\n",
    "\n",
    "# locating username and password inputs\n",
    "username = login_box.find_element_by_xpath('.//input[@name=\"session[username_or_email]\"]')\n",
    "password = login_box.find_element_by_xpath('.//input[@name=\"session[password]\"]')\n",
    "\n",
    "# sending text to the inputs\n",
    "username.send_keys(\"Write Email Here\")\n",
    "password.send_keys(\"Write Password Here\")\n",
    "# username.send_keys(os.environ.get(\"TWITTER_USER\"))\n",
    "# password.send_keys(os.environ.get(\"TWITTER_PASS\"))\n",
    "\n",
    "# locating login button and then clicking on it\n",
    "login_button = driver.find_element_by_xpath('//div[@role=\"button\"]')\n",
    "login_button.click()\n",
    "\n",
    "# closing driver\n",
    "# driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25270ac-6f79-4b51-b573-a10e6b6c7755",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- website -> https://www.adamchoi.co.uk/overs/detailed\n",
    "\n",
    "function main(splash, args)\n",
    "    -- If a website doesn't render correctly, disabling Private mode might help\n",
    "    splash.private_mode_enabled = false\n",
    "    -- Go to the URL set on the splash browser and then wait 3 seconds to let the page render\n",
    "    assert(splash:go(args.url))\n",
    "    assert(splash:wait(3))\n",
    "    -- Select all the elements that have the css selector \"label.btn.btn-sm.btn-primary\"\n",
    "    all_matches = assert(splash:select_all(\"label.btn.btn-sm.btn-primary\"))\n",
    "    -- Two elements were selected. We want to click on the second button, then wait 3 seconds to let the page render\n",
    "    all_matches[2]: mouse_click()\n",
    "    assert (splash:wait(3))\n",
    "    -- Increase the viewport to make all the content visible\n",
    "    splash: set_viewport_full()\n",
    "    return {splash: png(), splash: html()}\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5533b09b-d7ec-44eb-a83a-e14f308a73d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import Select\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# define the website to scrape and path where the chromediver is located\n",
    "website = 'https://www.adamchoi.co.uk/overs/detailed'\n",
    "path = '/Users/frankandrade/Downloads/chromedriver'  # write your path here\n",
    "service = Service(executable_path=path)  # selenium 4\n",
    "driver = webdriver.Chrome(service=service)  # define 'driver' variable\n",
    "# open Google Chrome with chromedriver\n",
    "driver.get(website)\n",
    "\n",
    "# locate and click on a button\n",
    "all_matches_button = driver.find_element(by='xpath', value='//label[@analytics-event=\"All matches\"]')\n",
    "all_matches_button.click()\n",
    "\n",
    "# select elements in the table\n",
    "matches = driver.find_elements(by='xpath', value='//tr')\n",
    "\n",
    "# storage data in lists\n",
    "date = []\n",
    "home_team = []\n",
    "score = []\n",
    "away_team = []\n",
    "\n",
    "# looping through the matches list\n",
    "for match in matches:\n",
    "    date.append(match.find_element(by='xpath', value='./td[1]').text)\n",
    "    home = match.find_element(by='xpath', value='./td[2]').text\n",
    "    home_team.append(home)\n",
    "    print(home)\n",
    "    score.append(match.find_element(by='xpath', value='./td[3]').text)\n",
    "    away_team.append(match.find_element(by='xpath', value='./td[4]').text)\n",
    "# quit drive we opened at the beginning\n",
    "driver.quit()\n",
    "\n",
    "# Create Dataframe in Pandas and export to CSV (Excel)\n",
    "df = pd.DataFrame({'date': date, 'home_team': home_team, 'score': score, 'away_team': away_team})\n",
    "df.to_csv('football_data.csv', index=False)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad63c6ca-b198-48d4-b78a-36e629f716b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# define the website to scrape and path where the chromediver is located\n",
    "website = 'https://www.adamchoi.co.uk/overs/detailed'\n",
    "path = '/Users/frank/Downloads/chromedriver' # write your path here\n",
    "# define 'driver' variable\n",
    "driver = webdriver.Chrome(path)\n",
    "# open Google Chrome with chromedriver\n",
    "driver.get(website)\n",
    "\n",
    "# locate and click on a button\n",
    "all_matches_button = driver.find_element_by_xpath('//label[@analytics-event=\"All matches\"]')\n",
    "all_matches_button.click()\n",
    "\n",
    "# select elements in the table\n",
    "matches = driver.find_elements_by_tag_name('tr')\n",
    "\n",
    "# storage data in lists\n",
    "date = []\n",
    "home_team = []\n",
    "score = []\n",
    "away_team = []\n",
    "\n",
    "# looping through the matches list\n",
    "for match in matches:\n",
    "    date.append(match.find_element_by_xpath('./td[1]').text)\n",
    "    home = match.find_element_by_xpath('./td[2]').text\n",
    "    home_team.append(home)\n",
    "    print(home)\n",
    "    score.append(match.find_element_by_xpath('./td[3]').text)\n",
    "    away_team.append(match.find_element_by_xpath('./td[4]').text)\n",
    "# quit drive we opened at the beginning\n",
    "driver.quit()\n",
    "\n",
    "# Create Dataframe in Pandas and export to CSV (Excel)\n",
    "df = pd.DataFrame({'date': date, 'home_team': home_team, 'score': score, 'away_team': away_team})\n",
    "df.to_csv('football_data.csv', index=False)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0267bdc5-6b81-42b6-8c09-71221923d57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "from scrapy.spiders import CrawlSpider, Rule\n",
    "\n",
    "\n",
    "class TranscriptsSpider(CrawlSpider):\n",
    "    name = 'transcripts'\n",
    "    allowed_domains = ['subslikescript.com']\n",
    "    start_urls = ['https://subslikescript.com/movies']\n",
    "\n",
    "    # Setting rules for the crawler\n",
    "    rules = (\n",
    "        Rule(LinkExtractor(restrict_xpaths=(\"//ul[@class='scripts-list']/a\")), callback='parse_item', follow=True)\n",
    "    )\n",
    "\n",
    "    def parse_item(self, response):\n",
    "        # Getting the article box that contains the data we want (title, plot, etc)\n",
    "        article = response.xpath(\"//article[@class='main-article']\")\n",
    "\n",
    "        # Extract the data we want and then yield it\n",
    "        yield {\n",
    "            'title': article.xpath(\"./h1/text()\").get(),\n",
    "            'plot': article.xpath(\"./p/text()\").get(),\n",
    "            'transcript': article.xpath(\"./div[@class='full-script']/text()\").getall(),\n",
    "            'url': response.url,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d082c77-fac3-47ae-a2a9-fb04d1d71c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "#################################################\n",
    "# Extracting a movie transcript\n",
    "#################################################\n",
    "\n",
    "# How To Get The HTML\n",
    "website = 'https://subslikescript.com/movie/Titanic-120338'\n",
    "result = requests.get(website)\n",
    "content = result.text\n",
    "soup = BeautifulSoup(content, 'lxml')\n",
    "# print(soup.prettify())  # prints the HTML of the website\n",
    "\n",
    "# Locate the box that contains title and transcript\n",
    "box = soup.find('article', class_='main-article')\n",
    "# Locate title and transcript\n",
    "title = box.find('h1').get_text()\n",
    "transcript = box.find('div', class_='full-script').get_text(strip=True, separator=' ')\n",
    "\n",
    "# Exporting data in a text file with the \"title\" name\n",
    "with open(f'{title}.txt', 'w') as file:\n",
    "    file.write(transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fee1e4-6942-4081-8df9-b87f7cd2bae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "\n",
    "class AudibleSpider(scrapy.Spider):\n",
    "    name = 'audible'\n",
    "    allowed_domains = ['www.audible.com']\n",
    "    start_urls = ['https://www.audible.com/search/']\n",
    "\n",
    "    def parse(self, response):\n",
    "        # Getting the box that contains all the info we want (title, author, length)\n",
    "        # product_container = response.xpath('//div[@class=\"adbl-impression-container \"]/li')\n",
    "        product_container = response.xpath('//div[@class=\"adbl-impression-container \"]//li[contains(@class, \"productListItem\")]')\n",
    "\n",
    "        # Looping through each product listed in the product_container box\n",
    "        for product in product_container:\n",
    "            book_title = product.xpath('.//h3[contains(@class, \"bc-heading\")]/a/text()').get()\n",
    "            book_author = product.xpath('.//li[contains(@class, \"authorLabel\")]/span/a/text()').getall()\n",
    "            book_length = product.xpath('.//li[contains(@class, \"runtimeLabel\")]/span/text()').get()\n",
    "\n",
    "            # Return data extracted\n",
    "            yield {\n",
    "                'title': book_title,\n",
    "                'author': book_author,\n",
    "                'length': book_length,\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d6ec1d-be93-4fa8-a2e7-935aebddcabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "\n",
    "web = \"https://www.audible.com/search\"\n",
    "path = '/Users/frankandrade/Downloads/chromedriver'\n",
    "driver = webdriver.Chrome(path)\n",
    "driver.get(web)\n",
    "driver.maximize_window()\n",
    "\n",
    "# Locating the box that contains all the audiobooks listed in the page\n",
    "container = driver.find_element_by_class_name('adbl-impression-container ')\n",
    "\n",
    "# Getting all the audiobooks listed (the \"/\" gives immediate child nodes)\n",
    "products = container.find_elements_by_xpath('.//li[contains(@class, \"productListItem\")]')\n",
    "# products = container.find_elements_by_xpath('./li')\n",
    "\n",
    "# Initializing storage\n",
    "book_title = []\n",
    "book_author = []\n",
    "book_length = []\n",
    "# Looping through the products list (each \"product\" is an audiobook)\n",
    "for product in products:\n",
    "    # We use \"contains\" to search for web elements that contain a particular text, so we avoid building long XPATH\n",
    "    book_title.append(product.find_element_by_xpath('.//h3[contains(@class, \"bc-heading\")]').text)  # Storing data in list\n",
    "    book_author.append(product.find_element_by_xpath('.//li[contains(@class, \"authorLabel\")]').text)\n",
    "    book_length.append(product.find_element_by_xpath('.//li[contains(@class, \"runtimeLabel\")]').text)\n",
    "\n",
    "driver.quit()\n",
    "# Storing the data into a DataFrame and exporting to a csv file\n",
    "df_books = pd.DataFrame({'title': book_title, 'author': book_author, 'length': book_length})\n",
    "df_books.to_csv('books.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
